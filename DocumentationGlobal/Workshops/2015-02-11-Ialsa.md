---
title: Possible Discussion Topics
author: Will Beasley & the IALSA team
output:
  html_document:
    keep_md: yes
    toc: yes
    toclevel: 2??????
---

Overall Goals
=================================================
This list is a menu of potential issues we might discuss together.  It's not a blue print.  There are a lot of bullets that have suggestions that may not be best for IALSA scenarios, but they probably discussed and addressed

REDCap
=================================================

### Inside Sweet spot
If your needs fit its sweet spot, life is good.  You'll get:

* Fast GUI development.
* Almost automatic security & logging to support HIPAA compliance.  This includes backup mechanisms and audit logs.
* Leverage existing (and possibly confederated) LDAP authentication.
* Various data entry options for public & private  visibility.
* One "server" that can host hundreds of "projects" (ie, databases).  Each has granular user privileges to isolate users' access between (and even within) projects.

To be in the sweet spot:

* The one-to-many relationships must all connect to the subject.  For IALSA contributors, I imagine the relationships will usally ebased on time.  REDCap's longitudinal capabilities work well when your protocol resembles a clinical research protocol.
* Either real-time data entry, or offline/disconnected entry that won't collide with other user's simultaneous writing.


### Outside Sweet spot
REDCap probably isn't the best tool for:

* Complex schemas with either (a) multiple nested one-to-many relationships, or (b) a lot of one-to-many relationships.
* Data warehouses, where it interacts only with computers.  If a human isn't either reading or writing to the database, probably better tools.
* Unconventional measurements & items.  It's not meant for cognitive items like eye tracking or sub-second reaction times.
* Datasets with a lot of churn.  If you're consistently deleting huge batches and rewriting, the log gets huge.

### Other Topics
* Conventional forms vs "Surveys"
* Options for connecting REDCap to other databases & workstations (manual, API, DET, DDP).
    * API makes the most sense for statisticians; our REDCapR package.

Warehouse 
=================================================
Architecture

* Conventional Relational models vs newer structures.
* OLAP instead of OLTP
    * OLTP when humans enter data *vs* OLAP when statisticians pull data.
    * OLTP for frequent transactions of a few records *vs* OLAP for infrequent transactions of large batches.
    * OLTP when a single lost record is hard/impossible to recover *vs* OLAP when you simply rerun the batch.
    * OLTP to improve latency *vs* OLAP to improve overall throughput.
* Multiple OLTPs can feed one OLAP (see Prairie Outpost architecture as an example)
* Incremental development, with an eye on the horizon.

Strategies

* Create different 'schemas' to represent the life-stage.  I typically use something like `import_raw`, `import_derived`, `aggregated`, `analysis`, `counts`.  The tables typically get smaller.  When  change is made, it's clearer what upstream tables don't have to be rerun.
     * `import_raw` is as close to the collaborator's extract as possible.  Where possible I'll hash PHI IDs (eg, SSN) and obfuscate PHI values (eg, month of birth).
     * `import_derived` has (a) clean & standardized variables, (b) fewer unnecessary columns, (c) fewer bad rows, and (d) some calculated variables (eg, age at appointment).
     * The other typically build off `import_derived`, or each other.
* Location: on campus vs in cloud
* Develop components to be as independent as possible.  Ideally a change doesn't require changes of everything downstream.
* Distribution & replication across nodes.
* Archive/backup strategies that (a) minimize data loss and (b) maximize reproducibility.
* Ability to find developers and statisticians to build & maintain the tools & frameworks used. Eg, favor a column-major database over Hadoop.


Reading & Writing to the Database
=================================================
* Sometimes called the DAL (data access layer) in enterprise architecture.

* All interaction should be programatic, not manual.
    * saves times over the long-run
    * documents the process for you and others

* Separate the code into layers/tiers to improve cognitive load and division of labor.  Don't require  an expensive PhD statistician to be involved in the database administration.  Don't require an even more expensive DBA to be involved in the analysis code.  At minimum create a layer for

    1. cleaning & validating the external data,
    2. importing into to your data warehouse (part of the DAL), 
    3. exporting from your data warehouse (part of the DAL),
    4. manipulating/combining/aggregating the data,
    5. exploring the manipulated data (eg, basic graphs and descriptives),
    6. analyzing/modeling the manipulated data (eg, CDA)
    
    The last two layers can be combined with the reporting code if they're executed quickly.  If it's a long-running Bayesian or SEM analysis, consider splitting the analyzing layer in two.  One calculates the expensive model and caches the results (in a serialized R object).  The first reads the pre-computed results and incorporates it into a report.

* Common Avenues
    * RODBC/JDBC for small & medium size datasets
    * Bulk CSV for large datasets.
        * You'll likely need to clean the incoming data, then save again as CSV)
        * This enables parallel cleaning (by different workstations/VMs) and parallel loading/unloading (on different warehouse nodes.)
    * API for specialized databases and application servers (eg, REDCap)
    
    The same warehouse can offer different avenues, such as bulk CSV uploads (when the incoming data is 10GB+) and RODBC exports (when the aggregated dataset is ~2MB).

Analysis
=================================================

### Hadleyverse/RStudio Suite

The Hadleyverse/RStudio set of tools is always my front runner because that team has a knack for developing tools that

* meet your needs, without being ornate or unnecessarily complicated,
* fit well with each other.  You can mix & match almost any combination of
    * knitr
    * Shiny
    * dplyr, plyr, reshape2, & tidyr
    * ggplot and ggvis
* fit well with outside technology, for example
    * Shiny and leaflet maps
    * Shiny and contemporary HTML5 & JavaScript approaches (eg, Bootstrap & d3)
    * knitr and most R packages
    * rmarkdown and Jekyll sites
* support reproducible research
    * the text-based files play nicely with GitHub and other version control systems (as opposed to binary files), 
    * their packages are very general, and equally applicable across the biology-psychology spectrum, and 
    * they encourage encapsulation of your functions into R packages
* obtain a strong following and great support on Stack Overflow and Google Groups.


### Automated Reports

Reporting Approaches

* "Dynamic" knitr reports 
    * Also consider        
        1. simple reports in REDCap, 
        2. database reporting frameworks with drill-down capabilities (eg, SQL Server Reporting Services), and even 
        3. Mail Merge in MS Word or LibreOffice.
    * For wide-spread or frequently refreshed reports, deploying through email or a internal fileserver isn't feasible.  You'll likely want a dedicated web site.  Consider how automated that website needs to be.
    * Separate the "model code" from the "presentation code" (to borrow a concept from enterprise architecture).  Allows you to have different looking reports using the same analysis code (eg, one for stakeholders, and one for nurses).
    * Likely paths:
        * LaTeX to PDF (for public/pretty reports on paper for external audiences)
        * markdown to html (for quick reports on a computer monitor for internal audiences)
        * markdown to PDF (for something in between?)

* "Interactive" Shiny reports
    * Also consider 
        1. Conventional HTML/PHP/Python/.NET sites using d3 and other JavaScript libraries
        2. all-flash-and-no-substance tools like Tableau (but not for too long)

### Publication-bound Modeling
Compared to automated reports:

* Efficiency and repetition is much less important,
* Self-documentation & reproducibility are still important, a
* Allowing ad-hoc directions is much more important. (Eg chasing anomolies and rabbit holes.  Validating suspicious patterns that weren't suspicious with an earlier dataset),

* External readers won't see the report itself.  But the report stillupdates all the tables & graphs that will be pasted into the manuscript.


Combining Info from Different Protocols
=================================================
* No such thing as a 'context-free' variable.  Even a simple measurement like "birth weight" or "glucose level"" can differ across two *well-designed* studies.  They two hospitals had different (but equally justifiable):
    1. Labs running the blood tests
    2. Durations before delivery
    3. Demographics cultures in the surrounding area
    4. Screening protocols --one location have most people the screen & thorough/accurate test, while the other didn't burden the moms who passed the quick screen.
    
    What hope does a psychological variable have without a good code book?  There's not a single behavioral variable simpler than glucose level or body fat proportion.
    
* Require a minimum level of documentation for *each* item.  My blind suggestions are:

    1. Exact item wording (both the stem and available options)
    2. Changes to the item over the life of the protocol.  This is particularly relevant for your longitudinal studies.
    3. Description of the overall sample.
    4. Description of which subjects in the sample received the item.  If it involves branching logic, be very specific.  CAT people, you're on your own. 
    
    Use footnotes or lookup tables if the info is heavily repetitive across items.  But still have a footnote marker beside each item.
    
    
* Confederate

* When different organizations are involved, the human challenges are almost always harder to solve that computer challenges.  Strive for a flexible system that can accommodate changing human challenges, instead of trying to get the humans to accommodate your software.  (This point isn't very profound in a room full of behavioral scientists.)

