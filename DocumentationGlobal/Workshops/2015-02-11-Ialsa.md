---
title: Possible Discussion Topics
author: Will Beasley & the IALSA team
output:
  html_document:
    keep_md: yes
    toc: yes
---

REDCap
=================================================

### Inside Sweet spot
If your needs fit its sweet spot, life is good.  You'll get:

* Fast GUI development.
* Almost automatic security & logging to support HIPAA compliance.  This includes backup mechanisms and audit logs.
* Leverage existing (and possibly confederated) LDAP authentication.
* Various data entry options for public & private  visibility.
* One "server" that can host hundreds of "projects" (ie, databases).  Each has granular user privileges to isolate users' access between (and even within) projects.

To be in the sweet spot:

* The one-to-many relationships must all connect to the subject.  For IALSA contributors, I imagine the relationships will usally ebased on time.  REDCap's longitudinal capabilities work well when your protocol resembles a clinical research protocol.
* Either real-time data entry, or offline/disconnected entry that won't collide with other user's simultaneous writing.


### Outside Sweet spot
REDCap probably isn't the best tool for:

* Complex schemas with either (a) multiple nested one-to-many relationships, or (b) a lot of one-to-many relationships.
* Data warehouses, where it interacts only with computers.  If a human isn't either reading or writing to the database, probably better tools.
* Unconventional measurements & items.  It's not meant for cognitive items like eye tracking or sub-second reaction times.
* Datasets with a lot of churn.  If you're consistently deleting huge batches and rewriting, the log gets huge.

### Other Topics
* Conventional forms vs "Surveys"
* Options for connecting REDCap to other databases & workstations (manual, API, DET, DDP).
    * API makes the most sense for statisticians; our REDCapR package.

Warehouse 
=================================================
Architecutre

* Conventional Relational models vs newer structures.
* OLTP when humans enter data
* OLAP when statisticians pull data
* Multiple OLTP can feed one OLAP (see Prairie Outpost architecture as an example)
* Incremental development, with an eye on the horizon.

Strategies

* Location: on campus vs in cloud
* Develop components to be as independent as possible.  Ideally a change doesn't require changes of everything downstream.
* Archive/backup strategies that (a) minimize data loss and (b) maximize reproducibility.
* Will you be able to find developers and statisticians to build & maintain the tools & frameworks used.

Analysis
=================================================

### Automated Reports
The Hadleyverse/RStudio set of tools is always my front runner because that team has a knack for developing tools that

* meet your needs, without being ornate or unnecessarily complicated,
* fit well with each other.  You can mix & match almost any combination of
    * knitr
    * Shiny
    * dplyr, plyr, reshape2, & tidyr
    * ggplot and ggvis
* fit well with outside technology, for example
    * Shiny and leaflet maps
    * Shiny and contemporary HTML5 & JavaScript approaches (eg, Bootstrap & d3)
    * knitr and most R packages
    * rmarkdown and Jekyll sites
* obtain a strong following and great support on Stack Overflow and Google Groups.

Reporting Approaches

* "Dynamic" knitr reports 
    * Also consider        
        1. simple reports in REDCap, 
        2. database reporting frameworks with drill-down capabilities (eg, SQL Server Reporting Services), and even 
        3. Mail Merge in MS Word or LibreOffice.
    * For wide-spread or frequently refreshed reports, deploying through email or a internal fileserver isn't feasible.  You'll likely want a dedicated web site.  Consider how automated that website needs to be.

* "Interactive" Shiny reports
    * Also consider 
        1. Conventional HTML/PHP/Python/.NET sites using d3 and other JavaScript libraries
        2. all-flash-and-no-substance tools like Tableau (but not for too long)

Combining Info from Different Protocols
=================================================
* No such thing as a 'context-free' variable.  Even a simple measurement like "birth weight" or "glucose level"" can differ across two *well-designed* studies.  They two hospitals had different (but equally justifiable):
    1. Labs running the blood tests
    2. Durations before delivery
    3. Demographics of the surrounding area
    4. Screening protocols --one location have most people the screen & thorough/accurate test, while the other didn't burden the moms who passed the quick screen.
    
    What hope does a psychological variable have without a good code book?  There's not a single behavioral variable simpler than glucose level or body fat proportion.
    
* Require a minimum level of documentation for *each* item including
    1. Exact item wording (both the stem and available options)
    2. Changes to the item over the life of the protocol.  This is particularly relevant for your longitudinal studies.
    3. Description of the overall sample.
    4. Description of which subjects in the sample received the item.  If it involves branching logic, be very specific.  CAT people, you're on your own. 
    
    Use footnotes or lookup tables if the info is heavily repetitive across items.  
    
    
* Confederate

* When different organizations are involved, the human challenges are almost always harder to solve that computer challenges.  Strive for a flexible system that can accommodate changing human challenges, instead of trying to get the humans to accommodate your software.  (This point isn't very profound in a room full of behavioral scientists.)

